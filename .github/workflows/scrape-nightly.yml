name: 'Scrape usage data (nightly)'

on:
  workflow_dispatch:
  schedule:
    - cron: '05 0 * * *' # run daily at 5 minutes past midnight

jobs:
  main:
    defaults:
      run:
        working-directory: ./backend
    name: main
    runs-on: ubuntu-latest
    steps:
      - name: checkout code
        uses: actions/checkout@v4.1.7
      - name: setup uv
        uses: eifinger/setup-uv@v1
        with:
          version: 0.4.0
          enable-cache: 'true'
          cache-dependency-glob: 'backend/uv.lock'
      - name: cache chromedriver
        id: cache-chromedriver
        uses: actions/cache@v4
        with:
          key: chromedriver-${{ runner.os }}-${{ hashFiles('usr/local/bin/chromedriver') }}
          path: usr/local/bin/chromedriver
      - name: install chromedriver
        if: steps.cache-chromedriver.outputs.cache-hit != 'true'
        uses: nanasess/setup-chromedriver@v2
      - name: scrape data and upload to vercel KV
        env:
          SWITCH2_USERNAME: ${{secrets.switch2_username}}
          SWITCH2_PASSWORD: ${{secrets.switch2_password}}
          KV_REST_API_TOKEN: ${{secrets.kv_rest_api_token}}
          KV_REST_API_URL: ${{secrets.kv_rest_api_url}}
        run: uv run scrape.py
